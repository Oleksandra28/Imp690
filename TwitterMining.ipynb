{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10008\n",
      "[u'tweet id', u' tweet', u' label']\n",
      "{u' tweet': u\"I've got enough candles to supply a Mexican family\", u' label': u'off-topic', u'tweet id': u\"'262596552399396864'\"}\n"
     ]
    }
   ],
   "source": [
    "#1. Remove non-printable, ASCII characters\n",
    "\n",
    "import unicodecsv\n",
    "import tensorflow\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "disaster_file = \"2012_Sandy_Hurricane-ontopic_offtopic.csv\"\n",
    "\n",
    "with open(disaster_file, 'rb') as f:\n",
    "    reader = unicodecsv.DictReader(f)\n",
    "    disaster_header = reader.fieldnames\n",
    "    disaster_data = list(reader)\n",
    "    \n",
    "print len(disaster_data)\n",
    "print disaster_header\n",
    "for row in disaster_data:\n",
    "    print row \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: {u' tweet': u\"I've got enough candles to supply a Mexican family\", u' label': u'off-topic', u'tweet id': u\"'262596552399396864'\"}\n",
      "preprocessed_tweet: [u\"I've\", u'got', u'enough', u'candles', u'to', u'supply', u'a', u'Mexican', u'family']\n",
      "preprocessed_disaster_data\n",
      "[{u' tweet': '[u\"I\\'ve\", u\\'got\\', u\\'enough\\', u\\'candles\\', u\\'to\\', u\\'supply\\', u\\'a\\', u\\'Mexican\\', u\\'family\\']', u' label': u'off-topic', u'tweet id': u\"'262596552399396864'\"}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#from str import join\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def get_tokens(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def preprocess(tweet):\n",
    "    result = get_tokens(tweet)\n",
    "    \n",
    "    return result\n",
    "\n",
    "preprocessed_disaster_data = list()\n",
    "for row in disaster_data:\n",
    "    print \"row:\", row\n",
    "    preprocessed_tweet = preprocess(row[' tweet'])\n",
    "    ' '.join(preprocessed_tweet)\n",
    "    \n",
    "    print \"preprocessed_tweet:\", preprocessed_tweet\n",
    "    \n",
    "    new_row = {u'tweet id': row['tweet id'], \n",
    "               u' tweet': str(preprocessed_tweet),\n",
    "               u' label': row[' label']}\n",
    "    \n",
    "    preprocessed_disaster_data.append(new_row) \n",
    "    break\n",
    "    \n",
    "print \"preprocessed_disaster_data\"\n",
    "print preprocessed_disaster_data\n",
    "\n",
    "preprocessed_disaster_file = \"Preprocessed_\"+disaster_file\n",
    "with open(preprocessed_disaster_file, 'wb') as f:\n",
    "    writer = unicodecsv.DictWriter(f, fieldnames=disaster_header)\n",
    "    # show the header fields\n",
    "    writer.writeheader()\n",
    "    #d = {u'tweet id': 5387568, u' tweet':\"lol\", u' label': 3857957}\n",
    "    #writer.writerow(d)\n",
    "    writer.writerows(preprocessed_disaster_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
